{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Assignment Number\n",
    "\n",
    "To load the correct settings, the number for the current assignment is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT_NR = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.requester import Requester\n",
    "from canvas_connector.utils.canvas_utils import download_assignment_submissions\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Local imports\n",
    "from scripts.canvas_utils import update_canvas_grade, post_canvas_comments\n",
    "from scripts.jsonify import jsonify, jsonify_resources, analyze_jsonify_results\n",
    "from scripts.utils import ensure_folder_exists, create_file_list, parsed_submissions_quality_check, deduplicate_files_with_manual_fixes, load_latest_jsonified_student_submission, load_jsonified_resources\n",
    "from scripts.llm_utils import create_openai_message, prompt_gpt, format_with_default, format_and_compile_openai_messages\n",
    "from scripts.utils import extract_html_content, get_sum_points_for_pattern, get_weighted_points, deduplicate_highest_attempt\n",
    "from scripts.llm_report_utils import start_report_with_header, add_messages_to_report, add_text_to_report, add_prompt_and_response_to_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global settings\n",
    "from scripts.settings import *\n",
    "\n",
    "# Load assignment specific settings\n",
    "ASSIGNMENT = ASSIGNMENTS[ASSIGNMENT_NR]\n",
    "ASSIGNMENT_ID = ASSIGNMENT[\"canvas\"][\"assignment_id\"]\n",
    "QUIZ_ID = ASSIGNMENT[\"canvas\"][\"quiz_id\"]\n",
    "R_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"r_quiz_question_id\"]\n",
    "ADV_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"adv_quiz_question_id\"]\n",
    "LOCK_GRADES_DATE = ASSIGNMENT[\"lock_grades_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Canvas API\n",
    "canvas_client = Canvas(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "canvas_requester = Requester(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "\n",
    "# Initialize OpenAI API\n",
    "if USE_UVA_OPENAI:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"UVA_OPENAI_API_KEY\"), \n",
    "                           base_url=os.getenv(\"UVA_OPENAI_BASE_URL\"))\n",
    "    if MODEL == \"gpt-4o\":\n",
    "        MODEL = \"gpt4o\" # OpenAI API uses a different model name\n",
    "else:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jsonify Resources\n",
    "\n",
    "To ensure the latest changes to rubrics, assignment, example solutions, or goals are captured, the resources are jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_jsonify_results(jsonify_resources(ASSIGNMENT_NR, RESOURCES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Submissions\n",
    "\n",
    "All assignment submissions are downloaded and jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download assignment submissions\n",
    "user_whitelist = [513294]\n",
    "user_blacklist = []\n",
    "out_paths = download_assignment_submissions(canvas_requester, COURSE_ID, ASSIGNMENT_ID, user_whitelist, user_blacklist)\n",
    "\n",
    "# Jsonify submissions\n",
    "for out_path in out_paths:\n",
    "    jsonify(out_path, \".\".join(out_path.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some submissions may be formated incorrectly, despite instructing students how to format them and to validate them here before submitting: https://lukekorthals.shinyapps.io/pips-submission-validator/ \n",
    "\n",
    "Therefore, perform a quality check to make sure submissions were correctly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_check_df = parsed_submissions_quality_check(ASSIGNMENT_NR, ASSIGNMENT_ID)\n",
    "\n",
    "print(f\"Found\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"all_indicators_found\"]])} complete submissions\")\n",
    "print(f\"- {len(quality_check_df[~quality_check_df[\"all_indicators_found\"]])} incomplete submissions\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"contains_additional_indicators\"]])} submissions with additional indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with missing indicators to check if they are really missing or just not recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[~quality_check_df[\"all_indicators_found\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with additional indicators and see if you udnerstand what went wrong and if you can fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[quality_check_df[\"contains_additional_indicators\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to fix anything (e.g., because a student wrote #R 1 instead of #R1), copy the raw submission and append `_ManualFixes` before the file extension. Then rejsonify the manual fixes. The remainder of the pipeline will prefer files with ManualFixes over raw files. \n",
    "\n",
    "After jsonifying any files with ManualFixes, recheck the `quality_check_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonify submissions with manual fixes\n",
    "files_with_fixes = create_file_list(SUBMISSIONS_PATH, [\"_ManualFixes\"],[\".json\"])\n",
    "for file in files_with_fixes:\n",
    "    jsonify(file, \".\".join(file.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt LLM for Grading and Feedback\n",
    "The assignments of all students are graded and feedbacked by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user IDs\n",
    "user_ids = [user.split(\"-\")[1] for user in os.listdir(SUBMISSIONS_PATH) if user.startswith(\"user\")]\n",
    "\n",
    "# Get jsonified resources for this week\n",
    "resources = load_jsonified_resources(ASSIGNMENT_NR, \n",
    "                                     RESOURCES_PATH, \n",
    "                                     [\"questions\", \"solutions\", \"rubrics\", \"goals\", \"weights\"])\n",
    "\n",
    "# Load llm completion report templates\n",
    "header_template = open(\"resources/llm_report/llm_report_header_template.txt\", \"r\").read()\n",
    "\n",
    "# Prepare unformatted messages\n",
    "unformatted_grading_messages = [(\"system\", PROMPTS[\"grading\"][\"system_prompt\"]), \n",
    "                                (\"user\", PROMPTS[\"grading\"][\"user_prompt\"])]\n",
    "unformatted_feedback_qw_messages = [(\"system\", PROMPTS[\"feedback_questionwise\"][\"system_prompt\"]), \n",
    "                                    (\"user\", PROMPTS[\"feedback_questionwise\"][\"user_prompt\"])]\n",
    "unformatted_feedback_sum_messages = [(\"system\", PROMPTS[\"feedback_summary\"][\"system_prompt\"]), \n",
    "                                     (\"user\", PROMPTS[\"feedback_summary\"][\"user_prompt\"])]\n",
    "\n",
    "# Loop over all users\n",
    "user_ids = [\"513294\"]\n",
    "for user_id in user_ids:\n",
    "    if user_id != \"513294\":\n",
    "        continue\n",
    "\n",
    "    # Get student submission\n",
    "    submission, attempt = load_latest_jsonified_student_submission(ASSIGNMENT_ID, user_id, SUBMISSIONS_PATH)\n",
    "\n",
    "    # Initilize dicts\n",
    "    grading_dict = {}\n",
    "    feedback_dict = {}\n",
    "\n",
    "    # Initialize report\n",
    "    llm_report_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMCompletionReport.md\"\n",
    "    add_text_to_report(llm_report_out_path,\n",
    "                       text=format_with_default(header_template,\n",
    "                                                {\"model\": MODEL,\n",
    "                                                 \"grading_temperature\": GRADING_TEMPERATURE,\n",
    "                                                 \"feedback_temperature\": FEEDBACK_TEMPERATURE,\n",
    "                                                 \"n_choices_grading\": N_CHOICES_GRADING,\n",
    "                                                 \"n_choices_feedback\": N_CHOICES_FEEDBACK,\n",
    "                                                 \"student_id\": user_id,\n",
    "                                                 \"assignment_id\": ASSIGNMENT_ID}),\n",
    "                        start_new=True)\n",
    "    # Loop over all questions\n",
    "    i = 0\n",
    "    for indicator in resources[\"questions\"]:\n",
    "        i += 1\n",
    "        if i > 200:\n",
    "            break\n",
    "        print(indicator)\n",
    "\n",
    "        # Extract relevant information\n",
    "        formatting_dict = {\n",
    "            \"task\": resources[\"questions\"][indicator],\n",
    "            \"solution\": resources[\"solutions\"][indicator],\n",
    "            \"rubric\": resources[\"rubrics\"][indicator],\n",
    "            \"answer\": \"\\n\".join(submission[indicator]),\n",
    "            \"goal\": resources[\"goals\"][indicator]\n",
    "        }\n",
    "        \n",
    "\n",
    "        # Prompt for grading\n",
    "        messages = format_and_compile_openai_messages(unformatted_grading_messages, formatting_dict)\n",
    "        pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-grading_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_GRADING,\n",
    "                                temperature=GRADING_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to grading dict\n",
    "        grading_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                          indicator,\n",
    "                                          \"Grading\",\n",
    "                                          messages,\n",
    "                                          completion)\n",
    "        \n",
    "        # Save grading to file\n",
    "        dat = pd.DataFrame({\n",
    "            \"user_id\": [user_id],\n",
    "            \"assignment_id\": [ASSIGNMENT_ID],\n",
    "            \"attempt\": [attempt],\n",
    "            \"grader\": [MODEL],\n",
    "            \"question\": [indicator],\n",
    "            \"points\": [float(extract_html_content(completion.choices[0].message.content, \"points\"))],\n",
    "            \"explanation\": [extract_html_content(completion.choices[0].message.content, \"explanation\")]\n",
    "        })\n",
    "        file_name = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/grading/grading_user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_grader-{MODEL}_que-{indicator}.csv\"\n",
    "        ensure_folder_exists(file_name)\n",
    "        dat.to_csv(file_name, index=False)\n",
    "        \n",
    "        # Prompt for feedback\n",
    "        messages = format_and_compile_openai_messages(unformatted_feedback_qw_messages, formatting_dict)\n",
    "        pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-feedback-questionwise_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_FEEDBACK,\n",
    "                                temperature=FEEDBACK_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to feedback dict\n",
    "        feedback_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                          None,\n",
    "                                          \"Feedback\",  \n",
    "                                          messages,\n",
    "                                          completion)\n",
    "        \n",
    "    # Prompt for feedback summary\n",
    "    feedback = \"\\n\\n\\n\".join([f\"{key}\\n{extract_html_content(value, 'feedback')}\" for key, value in feedback_dict.items()])\n",
    "    messages = format_and_compile_openai_messages(unformatted_feedback_qw_messages, {\"feedback\": feedback})\n",
    "    pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_prompt-feedback-summary_completion.pkl\"\n",
    "    completion = prompt_gpt(openai_client,\n",
    "                            MODEL, \n",
    "                            messages, \n",
    "                            pkl_out_path=pkl_out_path, \n",
    "                            n=N_CHOICES_FEEDBACK,\n",
    "                            temperature=FEEDBACK_TEMPERATURE)\n",
    "\n",
    "    # Add chat completions to report\n",
    "    add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                        \"Feedback Summary\",\n",
    "                                        \"Feedback\",\n",
    "                                        messages,\n",
    "                                        completion)\n",
    "\n",
    "    # Get LLM grade\n",
    "    # This calculation is specific to the PIPS 2025 course\n",
    "    points = {key: float(extract_html_content(value, \"points\")) for key, value in grading_dict.items()}\n",
    "    points_w = get_weighted_points(points, resources[\"weights\"])\n",
    "    points_r = round(get_sum_points_for_pattern(points_w, r\"#R(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_radv = round(get_sum_points_for_pattern(points_w, r\"#Radv(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_py = round(get_sum_points_for_pattern(points_w, r\"#Python(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_adv = points_radv if points_radv > 0 else points_py\n",
    "    used_adv = \"You were graded based on Radv.\" if points_radv > 0 else \"You were graded based on Python.\"\n",
    "    grade = round(points_r + points_adv, 2)\n",
    "\n",
    "    # Save grade\n",
    "    dat = pd.DataFrame({\"user\": [user_id],\n",
    "                        \"assignment\": [ASSIGNMENT_ID],\n",
    "                        \"attempt\": [attempt],\n",
    "                        \"grader\": [MODEL],\n",
    "                        **points,\n",
    "                        \"points_r\": [points_r], \n",
    "                        \"points_radv\": [points_radv], \n",
    "                        \"points_py\": [points_py], \n",
    "                        \"points_adv\": [points_adv], \n",
    "                        \"used_adv\": [used_adv],\n",
    "                        \"grade\": [grade]})\n",
    "    dat.to_csv(f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/grading/grading_user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_grader-{MODEL}_que-combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload LLM Grading and feedback to Canvas\n",
    "For all students who were graded for this assignment, the LLM generated grade is uploaded together with some predetermined comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Canvas objects \n",
    "course = canvas_client.get_course(COURSE_ID)\n",
    "assignment = course.get_assignment(ASSIGNMENT_ID)\n",
    "quiz = course.get_quiz(QUIZ_ID)\n",
    "quiz_submissions = [quiz_submission for quiz_submission in quiz.get_submissions()]\n",
    "\n",
    "# Load text for comments to canvas\n",
    "comment_preliminary_grade = open(\"resources/canvas_comments/canvas-comment_preliminary_grade.txt\", \"r\").read()\n",
    "comment_feedback_received = open(\"resources/canvas_comments/canvas-comment_feedback_received.txt\", \"r\").read()\n",
    "\n",
    "# Get grading files\n",
    "grading_files = create_file_list(SUBMISSIONS_PATH, [f\"ass-{ASSIGNMENT_ID}\", f\"grader-{MODEL}_que-combined.csv\"],[\".json\"])\n",
    "grading_files = deduplicate_highest_attempt(grading_files)\n",
    "\n",
    "for f in grading_files:\n",
    "    \n",
    "    user_id = int(re.compile(r\"user-(\\d+)\").search(f).group(1))\n",
    "    if datetime.today() >= datetime.strptime(LOCK_GRADES_DATE, \"%Y-%m-%d\") and str(user_id) != \"513294\":\n",
    "        print(\"WARNING GRADES ARE LOCKED AND NO UPDATES TO CANVAS ARE MADE!\")\n",
    "        continue\n",
    "\n",
    "    file_list_indidividual_questions = create_file_list(SUBMISSIONS_PATH, \n",
    "                                                        [f\"ass-{ASSIGNMENT_ID}\", f\"grader-{MODEL}\", f\"user-{user_id}\"],\n",
    "                                                        [\".json\", \"que-combined\"])\n",
    "    explanations = []\n",
    "    for file in file_list_indidividual_questions:\n",
    "        df = pd.read_csv(file)\n",
    "        explanations.append(f\"{df.question.values[0]}\\n{df.explanation.values[0]}\")\n",
    "    comment_explanation = \"Explanations for grading:\\n\\n\" + \"\\n\\n\".join(explanations)\n",
    "\n",
    "    dat = pd.read_csv(f)\n",
    "\n",
    "    canvas_submission = assignment.get_submission(user = user_id)\n",
    "\n",
    "    # Update Canvas grade\n",
    "    update_canvas_grade(user_id,\n",
    "                        R_QUIZ_QUESTION_ID,\n",
    "                        ADV_QUIZ_QUESTION_ID,\n",
    "                        quiz_submissions,\n",
    "                        dat.points_r.values[0],\n",
    "                        dat.points_adv.values[0],\n",
    "                        dat.used_adv.values[0],\n",
    "                        dat.grade.values[0],\n",
    "                        canvas_submission)\n",
    "\n",
    "    # Post comments with grade and feedback\n",
    "    post_canvas_comments(canvas_submission, comments=[comment_preliminary_grade, \n",
    "                                                      comment_explanation,\n",
    "                                                      comment_feedback_received])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
