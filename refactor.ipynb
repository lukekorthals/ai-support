{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Assignment Number\n",
    "\n",
    "To load the correct settings, the number for the current assignment is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT_NR = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.requester import Requester\n",
    "from canvas_connector.utils.canvas_utils import download_assignment_submissions\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Local imports\n",
    "from scripts.jsonify import jsonify, jsonify_resources, analyze_jsonify_results\n",
    "from scripts.utils import ensure_folder_exists, create_file_list, parsed_submissions_quality_check, deduplicate_files_with_manual_fixes, load_latest_jsonified_student_submission, load_jsonified_resources\n",
    "from scripts.llm_utils import create_openai_message, prompt_gpt, format_prompt\n",
    "from scripts.utils import extract_html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global settings\n",
    "from scripts.settings import *\n",
    "\n",
    "# Load assignment specific settings\n",
    "ASSIGNMENT = ASSIGNMENTS[ASSIGNMENT_NR]\n",
    "ASSIGNMENT_ID = ASSIGNMENT[\"canvas\"][\"assignment_id\"]\n",
    "QUIZ_ID = ASSIGNMENT[\"canvas\"][\"quiz_id\"]\n",
    "R_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"r_quiz_question_id\"]\n",
    "ADV_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"adv_quiz_question_id\"]\n",
    "LOCK_GRADES_DATE = ASSIGNMENT[\"lock_grades_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Canvas API\n",
    "canvas_client = Canvas(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "canvas_requester = Requester(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "\n",
    "# Initialize OpenAI API\n",
    "if USE_UVA_OPENAI:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"UVA_OPENAI_API_KEY\"), \n",
    "                           base_url=os.getenv(\"UVA_OPENAI_BASE_URL\"))\n",
    "    if MODEL == \"gpt-4o\":\n",
    "        MODEL = \"gpt4o\" # OpenAI API uses a different model name\n",
    "else:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jsonify Resources\n",
    "\n",
    "To ensure the latest changes to rubrics, assignment, example solutions, or goals are captured, the resources are jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: 28 (R: 18, Radv: 2, Python: 8)\n",
      "rubrics: 28 (R: 18, Radv: 2, Python: 8)\n",
      "solutions: 28 (R: 18, Radv: 2, Python: 8)\n",
      "goals: 28 (R: 18, Radv: 2, Python: 8)\n"
     ]
    }
   ],
   "source": [
    "analyze_jsonify_results(jsonify_resources(ASSIGNMENT_NR, RESOURCES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Submissions\n",
    "\n",
    "All assignment submissions are downloaded and jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download assignment submissions\n",
    "user_whitelist = [513294]\n",
    "user_blacklist = []\n",
    "out_paths = download_assignment_submissions(canvas_requester, COURSE_ID, ASSIGNMENT_ID, user_whitelist, user_blacklist)\n",
    "\n",
    "# Jsonify submissions\n",
    "for out_path in out_paths:\n",
    "    jsonify(out_path, \".\".join(out_path.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some submissions may be formated incorrectly, despite instructing students how to format them and to validate them here before submitting: https://lukekorthals.shinyapps.io/pips-submission-validator/ \n",
    "\n",
    "Therefore, perform a quality check to make sure submissions were correctly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "- 1 complete submissions\n",
      "- 0 incomplete submissions\n",
      "- 0 submissions with additional indicators\n"
     ]
    }
   ],
   "source": [
    "quality_check_df = parsed_submissions_quality_check(ASSIGNMENT_NR, ASSIGNMENT_ID)\n",
    "\n",
    "print(f\"Found\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"all_indicators_found\"]])} complete submissions\")\n",
    "print(f\"- {len(quality_check_df[~quality_check_df[\"all_indicators_found\"]])} incomplete submissions\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"contains_additional_indicators\"]])} submissions with additional indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with missing indicators to check if they are really missing or just not recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>found_indicators</th>\n",
       "      <th>missing_indicators</th>\n",
       "      <th>additional_indicators</th>\n",
       "      <th>all_indicators_found</th>\n",
       "      <th>contains_additional_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, found_indicators, missing_indicators, additional_indicators, all_indicators_found, contains_additional_indicators]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[~quality_check_df[\"all_indicators_found\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with additional indicators and see if you udnerstand what went wrong and if you can fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>found_indicators</th>\n",
       "      <th>missing_indicators</th>\n",
       "      <th>additional_indicators</th>\n",
       "      <th>all_indicators_found</th>\n",
       "      <th>contains_additional_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, found_indicators, missing_indicators, additional_indicators, all_indicators_found, contains_additional_indicators]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[quality_check_df[\"contains_additional_indicators\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to fix anything (e.g., because a student wrote #R 1 instead of #R1), copy the raw submission and append `_ManualFixes` before the file extension. Then rejsonify the manual fixes. The remainder of the pipeline will prefer files with ManualFixes over raw files. \n",
    "\n",
    "After jsonifying any files with ManualFixes, recheck the `quality_check_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonify submissions with manual fixes\n",
    "files_with_fixes = create_file_list(SUBMISSIONS_PATH, [\"_ManualFixes\"],[\".json\"])\n",
    "for file in files_with_fixes:\n",
    "    jsonify(file, \".\".join(file.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade and Feedback submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user IDs\n",
    "user_ids = [user.split(\"-\")[1] for user in os.listdir(SUBMISSIONS_PATH) if user.startswith(\"user\")]\n",
    "\n",
    "# Get jsonified resources for this week\n",
    "resources = load_jsonified_resources(ASSIGNMENT_NR, RESOURCES_PATH)\n",
    "\n",
    "# Loop over all users\n",
    "for user_id in user_ids:\n",
    "    if user_id != \"513294\":\n",
    "        continue\n",
    "\n",
    "    # Get student submission\n",
    "    submission = load_latest_jsonified_student_submission(ASSIGNMENT_ID, user_id, SUBMISSIONS_PATH)\n",
    "\n",
    "    # Initilize dicts\n",
    "    grading_dict = {}\n",
    "    feedback_dict = {}\n",
    "\n",
    "    # Loop over all questions\n",
    "    for indicator in resources[\"questions\"]:\n",
    "\n",
    "        # Extract relevant information\n",
    "        question = resources[\"questions\"][indicator]\n",
    "        solution = resources[\"solutions\"][indicator]\n",
    "        rubric = resources[\"rubrics\"][indicator]\n",
    "        answer = submission[indicator]\n",
    "        goal = resources[\"goals\"][indicator]\n",
    "\n",
    "        # Prompt for grading\n",
    "        formated_user_prompt = format_prompt(PROMPTS[\"grading\"][\"user_prompt\"], {\"task\": question, \"solution\": solution, \"rubric\": rubric, \"answer\": answer})\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"grading\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "        pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_reports/pickled_completions/user-{user_id}_assignment-{ASSIGNMENT_ID}_task-{indicator}_prompt-grading_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_GRADING,\n",
    "                                temperature=GRADING_TEMPERATURE)\n",
    "        \n",
    "        grading_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "        \n",
    "        # Prompt for feedback\n",
    "        formated_user_prompt = format_prompt(PROMPTS[\"feedback_questionwise\"][\"user_prompt\"], {\"task\": question, \"answer\": answer, \"goal\": goal})\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"feedback_questionwise\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "        pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_reports/pickled_completions/user-{user_id}_assignment-{ASSIGNMENT_ID}_task-{indicator}_prompt-feedback-questionwise_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_FEEDBACK,\n",
    "                                temperature=FEEDBACK_TEMPERATURE)\n",
    "        \n",
    "        feedback_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "        \n",
    "        break\n",
    "    \n",
    "    # Prompt for feedback summary\n",
    "    feedback = \"\\n\\n\\n\".join([f\"{key}\\n{extract_html_content(value, 'feedback')}\" for key, value in feedback_dict.items()])\n",
    "    formated_user_prompt = format_prompt(PROMPTS[\"feedback_summary\"][\"user_prompt\"], {\"feedback\": feedback})\n",
    "    messages = create_openai_message(\"system\", PROMPTS[\"feedback_summary\"][\"system_prompt\"])\n",
    "    messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "    pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_reports/pickled_completions/user-{user_id}_assignment-{ASSIGNMENT_ID}_prompt-feedback-summary_completion.pkl\"\n",
    "    completion = prompt_gpt(openai_client,\n",
    "                            MODEL, \n",
    "                            messages, \n",
    "                            pkl_out_path=pkl_out_path, \n",
    "                            n=N_CHOICES_FEEDBACK,\n",
    "                            temperature=FEEDBACK_TEMPERATURE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
