{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Assignment Number\n",
    "\n",
    "To load the correct settings, the number for the current assignment is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT_NR = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.requester import Requester\n",
    "from canvas_connector.utils.canvas_utils import download_assignment_submissions\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Local imports\n",
    "from scripts.canvas_utils import update_canvas_grade, post_canvas_comments\n",
    "from scripts.jsonify import jsonify, jsonify_resources, analyze_jsonify_results\n",
    "from scripts.utils import ensure_folder_exists, create_file_list, parsed_submissions_quality_check, deduplicate_files_with_manual_fixes, load_latest_jsonified_student_submission, load_jsonified_resources\n",
    "from scripts.llm_utils import create_openai_message, prompt_gpt, format_with_default\n",
    "from scripts.utils import extract_html_content, get_sum_points_for_pattern, get_weighted_points, deduplicate_highest_attempt\n",
    "from scripts.llm_report_utils import start_report_with_header, add_messages_to_report, add_text_to_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global settings\n",
    "from scripts.settings import *\n",
    "\n",
    "# Load assignment specific settings\n",
    "ASSIGNMENT = ASSIGNMENTS[ASSIGNMENT_NR]\n",
    "ASSIGNMENT_ID = ASSIGNMENT[\"canvas\"][\"assignment_id\"]\n",
    "QUIZ_ID = ASSIGNMENT[\"canvas\"][\"quiz_id\"]\n",
    "R_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"r_quiz_question_id\"]\n",
    "ADV_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"adv_quiz_question_id\"]\n",
    "LOCK_GRADES_DATE = ASSIGNMENT[\"lock_grades_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Canvas API\n",
    "canvas_client = Canvas(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "canvas_requester = Requester(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "\n",
    "# Initialize OpenAI API\n",
    "if USE_UVA_OPENAI:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"UVA_OPENAI_API_KEY\"), \n",
    "                           base_url=os.getenv(\"UVA_OPENAI_BASE_URL\"))\n",
    "    if MODEL == \"gpt-4o\":\n",
    "        MODEL = \"gpt4o\" # OpenAI API uses a different model name\n",
    "else:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jsonify Resources\n",
    "\n",
    "To ensure the latest changes to rubrics, assignment, example solutions, or goals are captured, the resources are jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: 28 (R: 18, Radv: 2, Python: 8)\n",
      "rubrics: 28 (R: 18, Radv: 2, Python: 8)\n",
      "solutions: 28 (R: 18, Radv: 2, Python: 8)\n",
      "goals: 28 (R: 18, Radv: 2, Python: 8)\n",
      "weights: 28 (R: 18, Radv: 2, Python: 8)\n"
     ]
    }
   ],
   "source": [
    "analyze_jsonify_results(jsonify_resources(ASSIGNMENT_NR, RESOURCES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Submissions\n",
    "\n",
    "All assignment submissions are downloaded and jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download assignment submissions\n",
    "user_whitelist = [513294]\n",
    "user_blacklist = []\n",
    "out_paths = download_assignment_submissions(canvas_requester, COURSE_ID, ASSIGNMENT_ID, user_whitelist, user_blacklist)\n",
    "\n",
    "# Jsonify submissions\n",
    "for out_path in out_paths:\n",
    "    jsonify(out_path, \".\".join(out_path.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some submissions may be formated incorrectly, despite instructing students how to format them and to validate them here before submitting: https://lukekorthals.shinyapps.io/pips-submission-validator/ \n",
    "\n",
    "Therefore, perform a quality check to make sure submissions were correctly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "- 1 complete submissions\n",
      "- 0 incomplete submissions\n",
      "- 0 submissions with additional indicators\n"
     ]
    }
   ],
   "source": [
    "quality_check_df = parsed_submissions_quality_check(ASSIGNMENT_NR, ASSIGNMENT_ID)\n",
    "\n",
    "print(f\"Found\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"all_indicators_found\"]])} complete submissions\")\n",
    "print(f\"- {len(quality_check_df[~quality_check_df[\"all_indicators_found\"]])} incomplete submissions\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"contains_additional_indicators\"]])} submissions with additional indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with missing indicators to check if they are really missing or just not recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>found_indicators</th>\n",
       "      <th>missing_indicators</th>\n",
       "      <th>additional_indicators</th>\n",
       "      <th>all_indicators_found</th>\n",
       "      <th>contains_additional_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, found_indicators, missing_indicators, additional_indicators, all_indicators_found, contains_additional_indicators]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[~quality_check_df[\"all_indicators_found\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with additional indicators and see if you udnerstand what went wrong and if you can fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>found_indicators</th>\n",
       "      <th>missing_indicators</th>\n",
       "      <th>additional_indicators</th>\n",
       "      <th>all_indicators_found</th>\n",
       "      <th>contains_additional_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, found_indicators, missing_indicators, additional_indicators, all_indicators_found, contains_additional_indicators]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[quality_check_df[\"contains_additional_indicators\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to fix anything (e.g., because a student wrote #R 1 instead of #R1), copy the raw submission and append `_ManualFixes` before the file extension. Then rejsonify the manual fixes. The remainder of the pipeline will prefer files with ManualFixes over raw files. \n",
    "\n",
    "After jsonifying any files with ManualFixes, recheck the `quality_check_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonify submissions with manual fixes\n",
    "files_with_fixes = create_file_list(SUBMISSIONS_PATH, [\"_ManualFixes\"],[\".json\"])\n",
    "for file in files_with_fixes:\n",
    "    jsonify(file, \".\".join(file.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt LLM for Grading and Feedback\n",
    "The assignments of all students are graded and feedbacked by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt_and_response_to_report(llm_report_out_path: str = None,\n",
    "                                      level_2_header: str = None,\n",
    "                                      details_label: str = \"Details\",\n",
    "                                      prompt_messages: list = None,\n",
    "                                      completion_messages: list = None,\n",
    "                                      ):\n",
    "    \n",
    "    # Add level 2 header\n",
    "    if level_2_header is not None:\n",
    "        add_text_to_report(llm_report_out_path, f\"## Question {level_2_header}\\n\")\n",
    "    \n",
    "    # Start details\n",
    "    add_text_to_report(llm_report_out_path, f\"<details>\\n\\t<summary>{details_label}</summary>\\n\\n\")\n",
    "    \n",
    "\n",
    "    # Add prompt messages\n",
    "    if prompt_messages is not None:\n",
    "        add_messages_to_report(llm_report_out_path, prompt_messages, header=\"#### Prompts\\n\")\n",
    "\n",
    "    # Add completion messages\n",
    "    if completion_messages is not None:\n",
    "        add_messages_to_report(llm_report_out_path, completion_messages, header=\"#### Completion Choices\\n\")\n",
    "\n",
    "    # End details\n",
    "    \n",
    "    add_text_to_report(llm_report_out_path, f\"\\n\\n</details>\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#R1\n",
      "#R2\n",
      "#R3\n",
      "#R4\n",
      "#R5\n",
      "#R6\n",
      "#R7\n",
      "#R8\n",
      "#R9\n",
      "#R10\n",
      "#R11\n"
     ]
    }
   ],
   "source": [
    "# Get user IDs\n",
    "user_ids = [user.split(\"-\")[1] for user in os.listdir(SUBMISSIONS_PATH) if user.startswith(\"user\")]\n",
    "\n",
    "# Get jsonified resources for this week\n",
    "resources = load_jsonified_resources(ASSIGNMENT_NR, \n",
    "                                     RESOURCES_PATH, \n",
    "                                     [\"questions\", \"solutions\", \"rubrics\", \"goals\", \"weights\"])\n",
    "\n",
    "# Load llm completion report templates\n",
    "header_template = open(\"resources/llm_report/llm_report_header_template.txt\", \"r\").read()\n",
    "\n",
    "# Loop over all users\n",
    "user_ids = [\"513294\"]\n",
    "for user_id in user_ids:\n",
    "    if user_id != \"513294\":\n",
    "        continue\n",
    "\n",
    "    # Get student submission\n",
    "    submission, attempt = load_latest_jsonified_student_submission(ASSIGNMENT_ID, user_id, SUBMISSIONS_PATH)\n",
    "\n",
    "    # Initilize dicts\n",
    "    grading_dict = {}\n",
    "    feedback_dict = {}\n",
    "\n",
    "    # Initialize report\n",
    "    llm_report_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMCompletionReport.md\"\n",
    "    add_text_to_report(llm_report_out_path,\n",
    "                       text=format_with_default(header_template,\n",
    "                                                {\"model\": MODEL,\n",
    "                                                 \"grading_temperature\": GRADING_TEMPERATURE,\n",
    "                                                 \"feedback_temperature\": FEEDBACK_TEMPERATURE,\n",
    "                                                 \"n_choices_grading\": N_CHOICES_GRADING,\n",
    "                                                 \"n_choices_feedback\": N_CHOICES_FEEDBACK,\n",
    "                                                 \"student_id\": user_id,\n",
    "                                                 \"assignment_id\": ASSIGNMENT_ID}),\n",
    "                        start_new=True)\n",
    "    # Loop over all questions\n",
    "    i = 0\n",
    "    for indicator in resources[\"questions\"]:\n",
    "        i += 1\n",
    "        if i > 200:\n",
    "            break\n",
    "        print(indicator)\n",
    "\n",
    "        # Extract relevant information\n",
    "        question = resources[\"questions\"][indicator]\n",
    "        solution = resources[\"solutions\"][indicator]\n",
    "        rubric = resources[\"rubrics\"][indicator]\n",
    "        answer = submission[indicator]\n",
    "        goal = resources[\"goals\"][indicator]\n",
    "\n",
    "        # Prompt for grading\n",
    "        formated_user_prompt = format_with_default(PROMPTS[\"grading\"][\"user_prompt\"], {\"task\": question, \"solution\": solution, \"rubric\": rubric, \"answer\": answer})\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"grading\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "        pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-grading_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_GRADING,\n",
    "                                temperature=GRADING_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to grading dict\n",
    "        grading_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                            indicator,\n",
    "                                            \"Grading\",\n",
    "                                            messages,\n",
    "                                            completion_messages)\n",
    "        \n",
    "        # Prompt for feedback\n",
    "        formated_user_prompt = format_with_default(PROMPTS[\"feedback_questionwise\"][\"user_prompt\"], {\"task\": question, \"answer\": answer, \"goal\": goal})\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"feedback_questionwise\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "        pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-feedback-questionwise_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_FEEDBACK,\n",
    "                                temperature=FEEDBACK_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to feedback dict\n",
    "        feedback_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                            None,\n",
    "                                            \"Feedback\",\n",
    "                                            messages,\n",
    "                                            completion_messages)\n",
    "        \n",
    "    # Prompt for feedback summary\n",
    "    feedback = \"\\n\\n\\n\".join([f\"{key}\\n{extract_html_content(value, 'feedback')}\" for key, value in feedback_dict.items()])\n",
    "    formated_user_prompt = format_with_default(PROMPTS[\"feedback_summary\"][\"user_prompt\"], {\"feedback\": feedback})\n",
    "    messages = create_openai_message(\"system\", PROMPTS[\"feedback_summary\"][\"system_prompt\"])\n",
    "    messages += create_openai_message(\"user\", formated_user_prompt)\n",
    "    pkl_out_path = f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_prompt-feedback-summary_completion.pkl\"\n",
    "    completion = prompt_gpt(openai_client,\n",
    "                            MODEL, \n",
    "                            messages, \n",
    "                            pkl_out_path=pkl_out_path, \n",
    "                            n=N_CHOICES_FEEDBACK,\n",
    "                            temperature=FEEDBACK_TEMPERATURE)\n",
    "\n",
    "    # Add chat completions to report\n",
    "    completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "    add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                        \"Feedback Summary\",\n",
    "                                        \"Feedback\",\n",
    "                                        messages,\n",
    "                                        completion_messages)\n",
    "\n",
    "    # Get LLM grade\n",
    "    points = {key: float(extract_html_content(value, \"points\")) for key, value in grading_dict.items()}\n",
    "    points_w = get_weighted_points(points, resources[\"weights\"])\n",
    "    points_r = round(get_sum_points_for_pattern(points_w, r\"#R(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_radv = round(get_sum_points_for_pattern(points_w, r\"#Radv(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_py = round(get_sum_points_for_pattern(points_w, r\"#Python(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_adv = points_radv if points_radv > 0 else points_py\n",
    "    used_adv = \"You were graded based on Radv.\" if points_radv > 0 else \"You were graded based on Python.\"\n",
    "    grade = round(points_r + points_adv, 2)\n",
    "\n",
    "    # Save grade\n",
    "    df = pd.DataFrame({**points, \n",
    "              \"points_r\": points_r, \n",
    "              \"points_radv\": points_radv, \n",
    "              \"points_py\": points_py, \n",
    "              \"points_adv\": points_adv, \n",
    "              \"used_adv\": used_adv,\n",
    "              \"grade\": grade}, \n",
    "              index=[user_id])\n",
    "    df.to_csv(f\"submissions/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_grader-llm_grades.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Canvas objects \n",
    "course = canvas_client.get_course(COURSE_ID)\n",
    "assignment = course.get_assignment(ASSIGNMENT_ID)\n",
    "quiz = course.get_quiz(QUIZ_ID)\n",
    "quiz_submissions = [quiz_submission for quiz_submission in quiz.get_submissions()]\n",
    "\n",
    "# Load text for comments to canvas\n",
    "comment_preliminary_grade = open(\"resources/canvas_comments/canvas-comment_preliminary_grade.txt\", \"r\").read()\n",
    "comment_feedback_received = open(\"resources/canvas_comments/canvas-comment_feedback_received.txt\", \"r\").read()\n",
    "\n",
    "# Get grading files\n",
    "grading_files = create_file_list(SUBMISSIONS_PATH, [f\"ass-{ASSIGNMENT_ID}\", \"grader-llm_grades.csv\"],[\".json\"])\n",
    "grading_files = deduplicate_highest_attempt(grading_files)\n",
    "\n",
    "for f in grading_files:\n",
    "    \n",
    "    user_id = int(re.compile(r\"user-(\\d+)\").search(f).group(1))\n",
    "    if datetime.today() >= datetime.strptime(LOCK_GRADES_DATE, \"%Y-%m-%d\") and user_id != \"513294\":\n",
    "        print(\"WARNING GRADES ARE LOCKED AND NO UPDATES TO CANVAS ARE MADE!\")\n",
    "        continue\n",
    "\n",
    "    dat = pd.read_csv(f)\n",
    "\n",
    "    canvas_submission = assignment.get_submission(user = user_id)\n",
    "\n",
    "    # Update Canvas grade\n",
    "    update_canvas_grade(user_id,\n",
    "                        R_QUIZ_QUESTION_ID,\n",
    "                        ADV_QUIZ_QUESTION_ID,\n",
    "                        quiz_submissions,\n",
    "                        dat.points_r.values[0],\n",
    "                        dat.points_adv.values[0],\n",
    "                        dat.used_adv.values[0],\n",
    "                        dat.grade.values[0],\n",
    "                        canvas_submission)\n",
    "\n",
    "    # Post comments with grade and feedback\n",
    "    post_canvas_comments(canvas_submission, \n",
    "                            comments=[comment_preliminary_grade, comment_feedback_received])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
