{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pipeline that is being run every week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the week\n",
    "WEEK_NUMBER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "from scripts.utils import read_files_to_dict\n",
    "\n",
    "\n",
    "with open(\"settings.yaml\") as f:\n",
    "    settings = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# Weeks\n",
    "WEEK = settings[\"weeks\"][WEEK_NUMBER]\n",
    "\n",
    "# Global canvas settings\n",
    "COURSE_ID = settings[\"global\"][\"canvas\"][\"course_id\"]\n",
    "ASSIGNMENT_ID = WEEK[\"canvas\"][\"assignment_id\"]\n",
    "QUIZ_ID = WEEK[\"canvas\"][\"quiz_id\"]\n",
    "R_QUIZ_QUESTION_ID = WEEK[\"canvas\"][\"r_quiz_question_id\"]\n",
    "ADV_QUIZ_QUESTION_ID = WEEK[\"canvas\"][\"adv_quiz_question_id\"]\n",
    "\n",
    "# Paths\n",
    "RESOURCES_PATH = settings[\"global\"][\"paths\"][\"resources\"]\n",
    "SUBMISSIONS_PATH = settings[\"global\"][\"paths\"][\"submissions\"]\n",
    "STUDENT_SUBMISSION_TEMPLATE = settings[\"global\"][\"paths\"][\"student_submission_template\"]\n",
    "STUDENT_SUBMISSION_JSON_TEMPLATE = settings[\"global\"][\"paths\"][\"student_submission_json_template\"]\n",
    "LLM_COMPLETION_REPORT_TEMPLATE = settings[\"global\"][\"paths\"][\"llm_completion_report_template\"]\n",
    "LLM_FEEDBACK_REPORT_TEMPLATE = settings[\"global\"][\"paths\"][\"llm_feedback_report_template\"]\n",
    "LLM_GRADING_REPORT_TEMPLATE = settings[\"global\"][\"paths\"][\"llm_grading_report_template\"]\n",
    "\n",
    "# LLM Settings\n",
    "MODEL = settings[\"global\"][\"llm\"][\"model\"]\n",
    "GRADING_TEMPERATURE = settings[\"global\"][\"llm\"][\"grading_temperature\"]\n",
    "FEEDBACK_TEMPERATURE = settings[\"global\"][\"llm\"][\"feedback_temperature\"]\n",
    "N_CHOICES_GRADING = settings[\"global\"][\"llm\"][\"n_choices_grading\"]\n",
    "N_CHOICES_FEEDBACK = settings[\"global\"][\"llm\"][\"n_choices_feedback\"]\n",
    "PROMPTS = {k: read_files_to_dict(settings[\"global\"][\"llm\"][\"prompts\"][k]) for k in settings[\"global\"][\"llm\"][\"prompts\"].keys()}\n",
    "GLOBAL_RUBRICS = WEEK[\"global_rubrics\"]\n",
    "LOCK_GRADES_DATE = WEEK[\"lock_grades_date\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get student submissions\n",
    "\n",
    "The most recent valid submission is downloaded for each student and jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.requester import Requester\n",
    "from canvas_connector.utils.canvas_utils import get_assignment_submissions_with_history, whitelist_submissions, assemble_canvas_file_submissions, get_most_recent_valid_submissions\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Local imports\n",
    "from scripts.filehandling import make_student_folders\n",
    "from scripts.jsonify import jsonify\n",
    "from scripts.utils import create_file_list\n",
    "\n",
    "\n",
    "# Get environment variables\n",
    "CANVAS_API_URL = os.getenv(\"CANVAS_API_URL\")\n",
    "CANVAS_API_KEY = os.getenv(\"CANVAS_API_KEY\")\n",
    "\n",
    "# Prepare connection to Canvas API\n",
    "canvas = Canvas(CANVAS_API_URL, CANVAS_API_KEY)\n",
    "requester = Requester(CANVAS_API_URL, CANVAS_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sort students by group and first do all A Group Students, then B Group Students?\n",
    "# TODO: Separate grading report from feedback report, first create grading for all, then feedback for A, then upload, then feedback for B\n",
    "def deduplicate_files_with_manual_fixes(file_list):\n",
    "\n",
    "    # Helper function to extract que and att identifiers from a filename\n",
    "    def extract_identifiers(filename):\n",
    "        match = re.search(r'que-(\\d+)_att-(\\d+)', filename)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2)\n",
    "        return None, None\n",
    "\n",
    "    # Dictionary to hold the preferred file for each identifier pair\n",
    "    preferred_files = {}\n",
    "\n",
    "    for file in file_list:\n",
    "        que, att = extract_identifiers(file)\n",
    "        if (que, att) == (None, None):\n",
    "            continue  # Skip files that don't match the pattern\n",
    "\n",
    "        if (que, att) not in preferred_files or \"ManualFixes\" in file:\n",
    "            # Update if no file is recorded yet or if this file has \"ManualFixes\"\n",
    "            preferred_files[(que, att)] = file\n",
    "\n",
    "    # Return the list of preferred files\n",
    "    return list(preferred_files.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all student submissions \n",
    "all_submissions = get_assignment_submissions_with_history(requester, COURSE_ID, ASSIGNMENT_ID)\n",
    "student_ids = list({submission.user_id for submission in all_submissions})\n",
    "\n",
    "# Loop student_ids\n",
    "for student_id in student_ids:\n",
    "    \n",
    "    # Submissions of the current student\n",
    "    student_submissions = whitelist_submissions(all_submissions, [student_id])\n",
    "    if len(student_submissions) == 0:\n",
    "        print(f\"Student {student_id} has no submissions\")\n",
    "        # TODO: Add log \n",
    "        continue\n",
    "\n",
    "    # Get canvas file submissions\n",
    "    file_submissions = assemble_canvas_file_submissions(student_submissions)\n",
    "\n",
    "    # Get all files of most recent (valid) submission\n",
    "    valid_submissions = get_most_recent_valid_submissions(file_submissions)\n",
    "    if len(valid_submissions) == 0:\n",
    "        print(f\"Student {student_id} has no valid submissions\") # TODO add log\n",
    "        continue\n",
    "    \n",
    "    # Download valid submissions\n",
    "    for submission in valid_submissions:\n",
    "        out_path = STUDENT_SUBMISSION_TEMPLATE.format(\n",
    "            student_id=student_id,\n",
    "            week=WEEK_NUMBER,\n",
    "            assignment_id=ASSIGNMENT_ID,\n",
    "            attempt=submission.attempt,\n",
    "            question_id=submission.question_id,\n",
    "            attachment_id=submission.attachment_id,\n",
    "            file_extension=submission.file_extension\n",
    "        )\n",
    "        # check if file already exists\n",
    "        submission.download(out_path)\n",
    "        out_path = out_path + submission.file_extension\n",
    "        if \".zip\" in out_path:\n",
    "            with zipfile.ZipFile(out_path) as zip_file:\n",
    "                for member in zip_file.namelist():\n",
    "                    filename = os.path.basename(member)\n",
    "                    # skip directories\n",
    "                    if not filename:\n",
    "                        continue\n",
    "                    if filename.startswith(\"._\"):\n",
    "                        continue\n",
    "                    if \".DS_Store\" in filename:\n",
    "                        continue\n",
    "                \n",
    "                    # copy file (taken from zipfile's extract)\n",
    "                    source = zip_file.open(member)\n",
    "                    target = open(out_path.replace(submission.file_extension, f\"_{filename}\"), \"wb\")\n",
    "                    with source, target:\n",
    "                        shutil.copyfileobj(source, target)\n",
    "            \n",
    "\n",
    "        # Jsonify valid submissions\n",
    "        submission_files = create_file_list(\"submissions\", [f\"stu-{student_id}\", f\"ass-{ASSIGNMENT_ID}\", f\"try-{submission.attempt}\"], indicators_neg=[\".json\", \".pdf\", \".md\", \".zip\", \".png\"])\n",
    "        submission_files = deduplicate_files_with_manual_fixes(submission_files)\n",
    "        out_path = STUDENT_SUBMISSION_JSON_TEMPLATE.format(\n",
    "            student_id=student_id, \n",
    "            week=WEEK_NUMBER,\n",
    "            assignment_id=ASSIGNMENT_ID,\n",
    "            attempt=submission.attempt\n",
    "        )\n",
    "        jsonify(submission_files, out_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all students have submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from scripts.utils import load_jsonified_resources\n",
    "\n",
    "def count_question_indicators_per_group(indicators: list, groups=[r\"#R(\\d+)\", r\"#Radv(\\d+)\", r\"#Python(\\d+)\"]):\n",
    "    counts = {group.replace(\"(\\d+)\", \"\"): len(re.compile(group).findall(\", \".join(indicators))) for group in groups}\n",
    "    return counts\n",
    "student_ids = [int(folder.replace(\"stu-\", \"\")) for folder in os.listdir(SUBMISSIONS_PATH) if folder.startswith(\"stu-\")]\n",
    "\n",
    "\n",
    "jsonified_resources = load_jsonified_resources(WEEK_NUMBER) # TODO: Check all keys and log if it is incorrect\n",
    "\n",
    "\n",
    "\n",
    "desired_counts = count_question_indicators_per_group(list(jsonified_resources[\"questions\"].keys()))\n",
    "desired_counts_keys = {f\"{k}_{v}\": [] for k,v in desired_counts.items()}\n",
    "completed_keys = {\"R_completed\": [], \"Radv_completed\": [], \"Python_completed\": []}\n",
    "quality_check_dat = pd.DataFrame({\"student_id\": [], **desired_counts_keys, \"submission_found\": [], **completed_keys})\n",
    "\n",
    "submission_files = create_file_list(SUBMISSIONS_PATH, [\"submission.json\", str(ASSIGNMENT_ID)])\n",
    "for student_id in student_ids:\n",
    "    student_found = False\n",
    "    for file in submission_files:\n",
    "        file_student_id = int(re.compile(r\"stu-(\\d+)\").search(file).group(1))\n",
    "        if file_student_id != student_id:\n",
    "            continue\n",
    "        submission_json = json.load(open(file))\n",
    "        counts = count_question_indicators_per_group(list(submission_json.keys()))\n",
    "        counts_row = {k: [v] for k, v in zip(desired_counts_keys.keys(), counts.values())}\n",
    "        completed = {k: [c == dc] for k, c, dc in zip(completed_keys.keys(), counts.values(), desired_counts.values())}\n",
    "        row = pd.DataFrame({\"student_id\": [student_id], **counts_row, \"submission_found\": [1], **completed})\n",
    "        quality_check_dat = pd.concat([quality_check_dat, row])\n",
    "        student_found = True\n",
    "        break\n",
    "    if not student_found:\n",
    "        row = pd.DataFrame({\"student_id\": student_id, **{k: [0] for k in desired_counts_keys.keys()}, \"submission_found\": [0], \"R_completed\": [0], \"Radv_completed\": [0], \"Python_completed\": [0]})\n",
    "        quality_check_dat = pd.concat([quality_check_dat, row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_with_submission = len(quality_check_dat.loc[quality_check_dat[\"submission_found\"] == 1])\n",
    "students_without_submission = len(quality_check_dat.loc[quality_check_dat[\"submission_found\"] == 0])\n",
    "students_with_submission_but_missing_indicators = len(quality_check_dat.loc[(quality_check_dat[\"submission_found\"] == 1) & ((quality_check_dat[\"R_completed\"] == 0) | (quality_check_dat[\"Radv_completed\"] == 0) | (quality_check_dat[\"Python_completed\"] == 0))])\n",
    "\n",
    "print(f\"{students_with_submission} students with submission\")\n",
    "print(f\"{students_without_submission} students without submission\")\n",
    "print(f\"{students_with_submission_but_missing_indicators} students with submission but missing indicators\")\n",
    "\n",
    "quality_check_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students who did not complete R\n",
    "quality_check_dat[(quality_check_dat[\"submission_found\"] == 1) & (quality_check_dat[\"R_completed\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students who did not complete either Radv or python\n",
    "quality_check_dat[(quality_check_dat[\"submission_found\"] == 1) & (quality_check_dat[\"Radv_completed\"] == 0) & (quality_check_dat[\"Python_completed\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students who did not complete Radv \n",
    "quality_check_dat[(quality_check_dat[\"submission_found\"] == 1) & (quality_check_dat[\"Radv_completed\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students who did not complete Python\n",
    "quality_check_dat[(quality_check_dat[\"submission_found\"] == 1) & (quality_check_dat[\"Python_completed\"] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "from openai import OpenAI\n",
    "from openai._types import NOT_GIVEN\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "from scripts.utils import load_jsonified_resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM settings\n",
    "UVA_OPENAI_API_KEY = os.getenv(\"UVA_OPENAI_API_KEY\")\n",
    "UVA_OPENAI_BASE_URL = os.getenv(\"UVA_OPENAI_BASE_URL\")\n",
    "MODEL = \"gpt4o\" # this is the correct name for UVA API\n",
    "openai_client = OpenAI(api_key=UVA_OPENAI_API_KEY, base_url=UVA_OPENAI_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_student_submission(student_id: int, week: int, submissions_path: str = \"submissions\"):\n",
    "    \"\"\"Loads a jsonified submission for a given student and week.\"\"\"\n",
    "    file_list = create_file_list(f\"{submissions_path}/stu-{student_id}/week-{week}\", [\"submission.json\"])\n",
    "    if len(file_list) == 0:\n",
    "        return False, None, None\n",
    "    file_list = sorted(file_list, key=lambda x: int(re.search(r'try-(\\d+)', x).group(1)))\n",
    "    submission = json.load(open(file_list[-1])) \n",
    "    submission = {k: \"\\n\".join(v) for k, v in submission.items()}\n",
    "    attempt = int(re.search(r'try-(\\d+)', file_list[-1]).group(1))\n",
    "    return True, submission, attempt\n",
    "\n",
    "def compare_dict_keys(dicts: list) -> Tuple[bool, set]:\n",
    "    \"\"\"Checks if the keys of a list of dictionaries are the same.\"\"\"\n",
    "    all_keys = {frozenset(d.keys()) for d in dicts}\n",
    "    if len(all_keys) > 1:\n",
    "        print(\"WARNING! Keys are not the same\")\n",
    "        return False, all_keys\n",
    "    return True, all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_report_with_header(report_path: str, model: str, grading_temperature: float, feedback_temperature: float, n_choices_grading: int, n_choices_feedback: int, student_id: int, week: int):\n",
    "    template = \"\"\"# LLM Prompt Report\n",
    "- model: {model}\n",
    "- grading_temperature: {grading_temperature}\n",
    "- feedback_temperature: {feedback_temperature}\n",
    "- n_choices_grading: {n_choices_grading}\n",
    "- n_choices_feedback: {n_choices_feedback}\n",
    "- student_id: {student_id}\n",
    "- week: {week}\\n\\n\"\"\"\n",
    "    text = template.format(model=model, \n",
    "                           grading_temperature=grading_temperature, \n",
    "                           feedback_temperature=feedback_temperature, \n",
    "                           n_choices_grading=n_choices_grading, \n",
    "                           n_choices_feedback=n_choices_feedback, \n",
    "                           student_id=student_id, week=week)\n",
    "    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def add_messages_to_report(report_path: str, messages: list, header: str = \"### Messages\\n\"):\n",
    "    \"\"\"Adds messages to a report.\"\"\"\n",
    "    template = \"\"\"<blockquote>\n",
    "<strong>{role}</strong>\n",
    "\n",
    "\\t{content}\n",
    "</blockquote>\\n\\n\"\"\"\n",
    "\n",
    "    text = header\n",
    "    for message in messages:\n",
    "        text += template.format(role=message[\"role\"], content=\"\\n\\t\".join(message[\"content\"].split(\"\\n\")))\n",
    "    with open(report_path, \"a\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def add_text_to_report(report_path: str, text: str):\n",
    "    with open(report_path, \"a\") as f:\n",
    "        f.write(text)\n",
    "    \n",
    "def remove_pseudo_html(text: str, tag: str):\n",
    "    return text.replace(f\"<{tag}>\", \"\").replace(f\"</{tag}>\", \"\")\n",
    "\n",
    "def get_majority_voted_points(choices: list):\n",
    "    points = [float(re.compile(r\"<points>(\\d+(?:\\.\\d+)?)</points>\").search(choice.message.content).group(1)) for choice in choices]\n",
    "    return max(set(points), key=points.count)\n",
    "\n",
    "def get_majority_voted_points_and_explanation(choices: list) -> Tuple[float, str]:\n",
    "    choice_dict = {}\n",
    "    points_list = []\n",
    "\n",
    "    # create a dictionary with points and explanation\n",
    "    for i, choice in enumerate(choices):\n",
    "        points = float(re.compile(r\"<points>(\\d+(?:\\.\\d+)?)</points>\").search(choice.message.content).group(1))\n",
    "        explanation = re.compile(r\"<explanation>(.*)</explanation>\", re.DOTALL).search(choice.message.content).group(1)\n",
    "        choice_dict[i] = {\"points\": points, \"explanation\": explanation}\n",
    "        points_list.append(points)\n",
    "\n",
    "\n",
    "    # find majority voted points\n",
    "    majority_voted_points = max(set(points_list), key=points_list.count)\n",
    "\n",
    "    # select the first explanation with the majority voted points\n",
    "    explanation = [v[\"explanation\"] for k, v in choice_dict.items() if v[\"points\"] == majority_voted_points][0]\n",
    "    return majority_voted_points, explanation\n",
    "\n",
    "def get_total_points_by_question_group(grading_dict, questions, groups = [r\"#R\\d+\", r\"#Radv\\d+\", r\"#Python\\d+\"]) -> dict:\n",
    "    total_points = {}\n",
    "    for group in groups:\n",
    "        possible_points = len([k for k in questions.keys() if re.match(group, k)])\n",
    "        achieved_points = sum([v[\"points\"] for k, v in grading_dict.items() if re.match(group, k)])\n",
    "        total_points[group.replace(r\"\\d+\", \"\")] = {\"possible_points\": possible_points, \"achieved_points\": achieved_points}\n",
    "    return total_points\n",
    "\n",
    "def get_preliminary_grade(total_points, calculations = {\"Radv\": {\"#R\": 0.8, \"#Radv\": 0.2}, \"Python\": {\"#R\": 0.8, \"#Python\": 0.2}}, max_grade = 10) -> float:\n",
    "    grade = 0\n",
    "    print(total_points)\n",
    "    percent = {k: v[\"achieved_points\"]/v[\"possible_points\"] for k, v in total_points.items()}\n",
    "    grade = {k: 0 for k in calculations.keys()}\n",
    "    for group, calc in calculations.items():\n",
    "        for k, v in calc.items():\n",
    "            grade[group] += percent[k] * v\n",
    "        grade[group] *= max_grade\n",
    "    # find which grade is higher and return that\n",
    "    return round(max(grade.values()), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.llm_utils import create_openai_message\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calculate_student_grade(dat, r_weight = 8, adv_py_weight = 2):\n",
    "    r_points = dat[dat['question'].str.startswith(r'#R') & ~dat['question'].str.startswith(r'#Radv')].points\n",
    "    r_adv_points = dat[dat['question'].str.startswith('#Radv')].points\n",
    "    py_points = dat[dat['question'].str.startswith('#Python')].points\n",
    "    weighted_r = sum(r_points) / len(r_points) * r_weight\n",
    "    weighted_r_adv = sum(r_adv_points) / len(r_adv_points) * adv_py_weight\n",
    "    weighted_py = sum(py_points) / len(py_points) * adv_py_weight\n",
    "    if sum(r_adv_points) > 0:\n",
    "        return round(weighted_r, 2), round(weighted_r_adv, 2), \"You were graded based on Radv.\"\n",
    "    return round(weighted_r, 2), round(weighted_py, 2), \"You were graded based on Python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH_PREPEND = \"NEW_PROMPTS_TEST/\"\n",
    "OUT_PATH_PREPEND = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jsonified resources\n",
    "jsonified_resources = load_jsonified_resources(WEEK_NUMBER) # TODO: Check all keys and log if it is incorrect\n",
    "\n",
    "# get assignment\n",
    "course = canvas.get_course(COURSE_ID)\n",
    "assignment = course.get_assignment(ASSIGNMENT_ID)\n",
    "quiz = course.get_quiz(QUIZ_ID)\n",
    "quiz_submissions = [quiz_submission for quiz_submission in quiz.get_submissions()]\n",
    "students_df = pd.read_csv(f\"{SUBMISSIONS_PATH}/students.csv\")\n",
    "\n",
    "# For each student\n",
    "for student_id in tqdm(student_ids):\n",
    "    print(f\"Processing student {student_id}\")\n",
    "\n",
    "    # Load jsonified submission\n",
    "    submission_found, submission, attempt = load_latest_student_submission(student_id, WEEK_NUMBER)\n",
    "    if not submission_found:\n",
    "        print(f\"Student {student_id} has no submission for week {WEEK_NUMBER}\") # TODO: Add log\n",
    "        continue\n",
    "\n",
    "    # Validate that all keys are the same\n",
    "    valid_keys, all_keys = compare_dict_keys([submission] + list(jsonified_resources.values()))\n",
    "    if not valid_keys:\n",
    "        print(f\"Student {student_id} has different keys in submission and resources for week {WEEK_NUMBER}\")\n",
    "        \n",
    "\n",
    "    # Prepare some dicts\n",
    "    grading_dict = {}\n",
    "    feedback_qw_dict = {}\n",
    "\n",
    "    # Initialize LLM report\n",
    "    llm_report_out_path = LLM_COMPLETION_REPORT_TEMPLATE.format(\n",
    "        student_id=student_id,\n",
    "        week=WEEK_NUMBER,\n",
    "        assignment_id=ASSIGNMENT_ID,\n",
    "        attempt=attempt\n",
    "    )\n",
    "\n",
    "    llm_report_out_path = OUT_PATH_PREPEND + llm_report_out_path\n",
    "\n",
    "    # Skip if the report exists\n",
    "    if os.path.exists(llm_report_out_path):\n",
    "        print(f\"Student {student_id} already has a report for their most recent attempt of week {WEEK_NUMBER}\")\n",
    "        continue\n",
    "\n",
    "    start_report_with_header(llm_report_out_path, \n",
    "                             MODEL, \n",
    "                             GRADING_TEMPERATURE, \n",
    "                             FEEDBACK_TEMPERATURE, \n",
    "                             N_CHOICES_GRADING, \n",
    "                             N_CHOICES_FEEDBACK, \n",
    "                             student_id, \n",
    "                             WEEK_NUMBER)\n",
    "\n",
    "    # Loop through questions\n",
    "    i = 1\n",
    "    for key in jsonified_resources[\"rubrics\"].keys():\n",
    "        if i > 200: # for testing\n",
    "            break\n",
    "        i += 1\n",
    "        # Add to report\n",
    "        add_text_to_report(llm_report_out_path, f\"## Question {key}\\n\")\n",
    "\n",
    "        # Get question, solution, rubric, goal, and answer\n",
    "        question = jsonified_resources[\"questions\"][key]\n",
    "        solution = jsonified_resources[\"solutions\"][key]\n",
    "        rubric = jsonified_resources[\"rubrics\"][key] + \"\\n\" + \"\\n\".join(GLOBAL_RUBRICS)\n",
    "        goal = jsonified_resources[\"goals\"][key]\n",
    "\n",
    "        if key not in submission:\n",
    "            answer = \"!Student did not attempt the task!\"\n",
    "        else:\n",
    "            answer = submission[key]\n",
    "\n",
    "\n",
    "        # Prepare the grading prompt\n",
    "        grading_content_prompt = PROMPTS[\"grading\"][\"user_prompt\"].format(\n",
    "            task=question, \n",
    "            solution=solution, \n",
    "            rubric=rubric, \n",
    "            answer=answer)\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"grading\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", grading_content_prompt)\n",
    "\n",
    "        # Add grading prompt to report\n",
    "        add_text_to_report(llm_report_out_path, f\"<details>\\n\\t<summary>Grading</summary>\\n\\n\")\n",
    "        add_messages_to_report(llm_report_out_path, messages, header=\"#### Prompts\\n\")\n",
    "\n",
    "        # Get LLM grading\n",
    "        completion = openai_client.chat.completions.create(model=MODEL, messages=messages, n=N_CHOICES_GRADING, temperature=GRADING_TEMPERATURE)\n",
    "        \n",
    "        # Add to grading dict\n",
    "        majority_points, majority_explanation = get_majority_voted_points_and_explanation(completion.choices) \n",
    "        grading_dict[key] = {\"points\": majority_points, \"explanation\": majority_explanation}\n",
    "\n",
    "        # Add completion to report \n",
    "        completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "        add_messages_to_report(llm_report_out_path, completion_messages, header=\"#### Completion Choices\\n\")\n",
    "        add_text_to_report(llm_report_out_path, f\"\\n\\n</details>\\n\\n\")\n",
    "\n",
    "        # Prepare the feedback prompt\n",
    "        feedback_qw_content_prompt = PROMPTS[\"feedback_questionwise\"][\"user_prompt\"].format(\n",
    "            task=question,\n",
    "            goal=goal,\n",
    "            answer=answer,\n",
    "            r=\"{r}\")\n",
    "        messages = create_openai_message(\"system\", PROMPTS[\"feedback_questionwise\"][\"system_prompt\"])\n",
    "        messages += create_openai_message(\"user\", feedback_qw_content_prompt)\n",
    "\n",
    "        # Add feedback prompt to report\n",
    "        add_text_to_report(llm_report_out_path, f\"<details>\\n\\t<summary>Feedback</summary>\\n\\n\")\n",
    "        add_messages_to_report(llm_report_out_path, messages, header=\"#### Prompts\\n\")\n",
    "\n",
    "        # Get LLM feedback\n",
    "        completion = openai_client.chat.completions.create(model=MODEL, messages=messages, n=N_CHOICES_FEEDBACK, temperature=FEEDBACK_TEMPERATURE)\n",
    "\n",
    "        # Add to feedback dict\n",
    "        feedback_qw_dict[key] = completion.choices[0].message.content # TODO: If we use multiple n this needs to change\n",
    "\n",
    "        # Add completion to report\n",
    "        completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "        add_messages_to_report(llm_report_out_path, completion_messages, header=\"#### Completion Choices\\n\")\n",
    "        add_text_to_report(llm_report_out_path, f\"\\n\\n</details>\\n\\n\")\n",
    "\n",
    "\n",
    "    # Feedback Summary\n",
    "    add_text_to_report(llm_report_out_path, f\"## Feedback Summary\\n\")\n",
    "\n",
    "    # Prepare the feedback summary prompt\n",
    "    feedback_sum_content_prompt = PROMPTS[\"feedback_summary\"][\"user_prompt\"].format(\n",
    "        feedback = \"\\n\\n\".join([f\"<question>{k}</question>\\n{v}\" for k, v in feedback_qw_dict.items()]))\n",
    "    messages = create_openai_message(\"system\", PROMPTS[\"feedback_summary\"][\"system_prompt\"])\n",
    "    messages += create_openai_message(\"user\", feedback_sum_content_prompt)\n",
    "\n",
    "    # Add feedback summary prompt to report\n",
    "    add_text_to_report(llm_report_out_path, f\"<details>\\n\\t<summary>Feedback</summary>\\n\\n\")\n",
    "    add_messages_to_report(llm_report_out_path, messages, header=\"#### Prompts\\n\")\n",
    "\n",
    "    # Get LLM feedback\n",
    "    completion = openai_client.chat.completions.create(model=MODEL, messages=messages, n=N_CHOICES_FEEDBACK, temperature=FEEDBACK_TEMPERATURE)\n",
    "\n",
    "    # Add completion to report\n",
    "    completion_messages = [{\"role\": choice.message.role, \"content\": choice.message.content} for choice in completion.choices]\n",
    "    add_messages_to_report(llm_report_out_path, completion_messages, header=\"#### Completion Choices\\n\")\n",
    "    add_text_to_report(llm_report_out_path, f\"\\n\\n</details>\\n\\n\")\n",
    "\n",
    "    # Create feedback report for student\n",
    "    pdf = MarkdownPdf()\n",
    "\n",
    "    # Header, summary, preliminary grade, coding challenge\n",
    "    section = f\"# Feedback Assignment {WEEK_NUMBER}\\n\\n\"\n",
    "    section += \"## Summary\\n\"\n",
    "    section += re.compile(r\"<summary>(.*)</summary>\", re.DOTALL).search(completion.choices[0].message.content).group(1)\n",
    "    section += \"\\n\\n\"\n",
    "    # section += f\"**Preliminary grade**: {calculate_preliminary_grade(grading_dict, jsonified_resources['questions'])}/10\\n\\n\"\n",
    "    section += \"## Coding Challenge\\n\"\n",
    "    section += \"We invite you to work on the following personalized coding challenge and submit your result on Canvas. Dont worry about being perfect, this is ungraded and just for your practice.\\n\\n\"\n",
    "    section += re.compile(r\"<coding-challenge>(.*)</?coding-challenge>\", re.DOTALL).search(completion.choices[0].message.content).group(1)\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Questionwise feedback\n",
    "    section = \"\"\n",
    "    for key, value in feedback_qw_dict.items():\n",
    "        section += f\"## {key}\\n\"\n",
    "        remove_idx = value.find(\"</my-thoughts>\") + len(\"</my-thoughts>\")\n",
    "        if remove_idx > len(\"</my-thoughts>\"):\n",
    "            value = value[remove_idx:]\n",
    "        remove_idx = value.find(\"<my-thoughts>\") + len(\"<my-thoughts>\")\n",
    "        if remove_idx > len(\"<my-thoughts>\"):\n",
    "            value = value[remove_idx:]\n",
    "        section += value\n",
    "        section += \"\\n\\n--\\n\\n\"\n",
    "\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Save pdf\n",
    "    student_feedback_report_out_path = LLM_FEEDBACK_REPORT_TEMPLATE.format(\n",
    "        student_id=student_id,\n",
    "        week=WEEK_NUMBER,\n",
    "        assignment_id=ASSIGNMENT_ID,\n",
    "        attempt=attempt\n",
    "    )\n",
    "    student_feedback_report_out_path = OUT_PATH_PREPEND + student_feedback_report_out_path\n",
    "    pdf.save(student_feedback_report_out_path)\n",
    "\n",
    "    # Create grading report for student\n",
    "    pdf = MarkdownPdf()\n",
    "\n",
    "    # Header, summary, preliminary grade, coding challenge\n",
    "    section = f\"# LLM Grading Assignment {WEEK_NUMBER}\\n\\n\"\n",
    "    total_points = get_total_points_by_question_group(grading_dict, jsonified_resources[\"questions\"])\n",
    "    preliminary_grade = get_preliminary_grade(total_points)\n",
    "    for group, points in total_points.items():\n",
    "        section += f\"**{group}:** {points['achieved_points']}/{points['possible_points']} points\\n\\n\"\n",
    "    section += f\"**Preliminary grade:** {preliminary_grade}/10\\n\\n\"\n",
    "\n",
    "    \n",
    "    # Questionwise grading\n",
    "    for key, value in grading_dict.items():\n",
    "        # section = \"\"\n",
    "        section += f\"## {key} ({value['points']}/1)\\n\"\n",
    "        section += value[\"explanation\"]\n",
    "        section += \"\\n\\n\"\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Save pdf\n",
    "    student_grading_report_out_path = LLM_GRADING_REPORT_TEMPLATE.format(\n",
    "        student_id=student_id,\n",
    "        week=WEEK_NUMBER,\n",
    "        assignment_id=ASSIGNMENT_ID,\n",
    "        attempt=attempt\n",
    "    )\n",
    "    student_grading_report_out_path = OUT_PATH_PREPEND + student_grading_report_out_path\n",
    "    pdf.save(student_grading_report_out_path)\n",
    "\n",
    "    if datetime.today() >= datetime.strptime(LOCK_GRADES_DATE, \"%Y-%m-%d\"):\n",
    "        print(\"WARNING GRADSE ARE LOCKED AND NO UPDATES TO CANVAS ARE MADE!\")\n",
    "        continue\n",
    "\n",
    "    # Create dataframe for grading\n",
    "    grading_df = pd.DataFrame(grading_dict).T.reset_index().rename(columns={\"index\": \"question\"})\n",
    "    missing_questions = set(jsonified_resources[\"questions\"].keys()) - set(grading_df[\"question\"])\n",
    "    missing_df = pd.DataFrame({\"question\": list(missing_questions), \"points\": 0, \"explanation\": \"Did not attempt\"})\n",
    "    grading_df = pd.concat([grading_df, missing_df], axis=0)\n",
    "\n",
    "    # calculate grade and create comment\n",
    "    r_grade, adv_grade, adv_grade_comment = calculate_student_grade(grading_df)\n",
    "    total_grade = r_grade + adv_grade\n",
    "\n",
    "\n",
    "    # Get the quiz_submission for this student\n",
    "    for quiz_submission in quiz_submissions:\n",
    "        if quiz_submission.user_id == int(student_id):\n",
    "            break\n",
    "    \n",
    "    # Get the canvas submission for this student\n",
    "    canvas_submission = assignment.get_submission(user = student_id)\n",
    "    \n",
    "    # Generate data for updating the score and comments\n",
    "    if R_QUIZ_QUESTION_ID == ADV_QUIZ_QUESTION_ID:\n",
    "        data = [{\"attempt\": canvas_submission.attempt,\n",
    "                 \"questions\": {str(R_QUIZ_QUESTION_ID): {\"score\": total_grade, \"comment\": adv_grade_comment}}}]\n",
    "    else:\n",
    "        data = [{\"attempt\": canvas_submission.attempt,\n",
    "                 \"questions\": {\n",
    "                     str(R_QUIZ_QUESTION_ID): {\"score\": r_grade},\n",
    "                     str(ADV_QUIZ_QUESTION_ID): {\"score\": adv_grade, \"comment\": adv_grade_comment}}}]\n",
    "\n",
    "    # Update the question scores and comments\n",
    "    quiz_submission.update_score_and_comments(quiz_submissions=data)\n",
    "\n",
    "    # Post a general comment\n",
    "    comment = \"\"\"Be aware that this is a preliminary grade (and comment) based on an automated process.\n",
    "We provide this to ensure you get a feeling for how last weeks assignment went as fast as possible.\n",
    "\n",
    "However, the automated system can make mistakes. \n",
    "Therefore, we will grade your assignment independently and your final grade may differ.\"\"\"\n",
    "    canvas_submission.edit(comment = {\"text_comment\": comment})\n",
    "\n",
    "    # Upload the feedback report if student is in group A\n",
    "    group = students_df.loc[students_df[\"user_id\"] == student_id, f\"week-{WEEK_NUMBER}\"].values[0]\n",
    "    if group == \"A\":\n",
    "        group_a_comment = \"\"\"This week you received personalized feedback on your assignment. \n",
    "\n",
    "It consists of a summary, an optional coding challenge and questionwise feedback. \n",
    "\n",
    "We encourage you to use this feedback to reflect on your assignment and to try working on the optional coding challenge.\"\"\"\n",
    "        canvas_submission.edit(comment = {\"text_comment\": group_a_comment})\n",
    "        canvas_submission.upload_comment(file=student_feedback_report_out_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
